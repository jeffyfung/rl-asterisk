# Deep Reinforcement Learning on Lunar Lander Environment

The problem considered involves optimising the trajectory of a Lunar Lander implemented using OpenAI Gym’s LunarLander-v2 environment (Oleg Klimov, 2023). The aim of this environment is to land the Lunar Lander on its landing pad by employing different throttle inputs. The environment was chosen to have a continuous observation space. The state space of the environment consists of an 8-dimensional vector: The first and the second elements are the x and y coordinates of the Lunar Lander respectively. The third and fourth elements are the linear x and y velocities of the Lunar Lander respectively. The fifth element is the angle in radians of the Lunar Lander. The sixth element is the angular velocity of the Lunar Lander. The seventh and eighth elements are booleans representing whehter each leg of the Lunar Lander is in contact with the ground or not.

Every action involving throttle input is considered to be at full throttle. The action space of the environment is at ∈ {0, 1, 2, 3}, representing the following 4 discrete actions: 0 - Do nothing, 1 -Fire left orientation engine, 2 - Fire main engine, 3 - Fire right orientation engine. The lander is modelled as a 2-dimensional dynamic body using the Box2D physics engine. This establishes a non-zero mass allowing it to be subject to forces. Each state captures the variables of interest for a single frame of the simulated lander environment at time t. The time step Δt = t2 − t1 is used to calculate the new state by integrating the equations of motion over Δt (Catto, 2013). We distinguish following four forces on the lander affecting the transition dynamics: 1) gravity is a constant force in the global negative y axis. 2) Firing the main thruster exerts a positive force along the local y axis. 3) Firing the left and right orientation thrusters produce opposite torque on the lander. 4) Collision with the moon surface (the moon is modeled as a static body with infinite mass). The landing pad is always located at x and y coordinates (0, 0) and the lunar lander starts each episode at the top centre of the view point. An initial random force is applied to the lander before the start of every episode to determine the first frame. This is handled by the environment as a simple extension of the equations of motions over a time step. The application of a random force has the effect of producing a stochastic environment. After every step in an episode a reward is given to the agent. Each reward will depend on the following factors: 1) The distance between the lander and the landing pad. 2) The speed of the lander. 3) The angle of the lander. 4) The angular acceleration of the lander. 5) A slight penalty is applied for using the thrusters. Furthermore upon episode termination a large positive reward is given for a safe landing or otherwise a large negative one. For further details on rewards consult Appendix A. An episode will terminate if either of the following conditions are met: 1) The lander crashes and the body of the lander is in contact with the surface of the moon (This does not include contact between either of the legs of the lander and the moon). 2) The lander leaves the viewpoint. 3) The lander is not "awake" i.e. it is not in motion and it does not collide with any other bodies.
